% This file was created with JabRef 2.9.2.
% Encoding: UTF8

@MISC{Bastien2012,
  author = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan
	and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and
	Bouchard, Nicolas and Bengio, Yoshua},
  title = {Theano: new features and speed improvements},
  howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},
  year = {2012},
  abstract = {Theano is a linear algebra compiler that optimizes a user’s symbolically-speciﬁed
	mathematical computations to produce efﬁcient low-level implementations.
	In this paper, we present new features and efﬁciency improvements
	to Theano, and benchmarks demonstrating Theano’s performance relative
	to Torch7, a recently introduced machine learning library, and to
	RNNLM, a C++ library targeted at recurrent neural networks.},
  owner = {sschneider},
  timestamp = {2015.09.30}
}

@OTHER{Bengio2007,
  author = {Yoshua Bengio and Olivier Delalleau},
  file = {:home/stes/thesis/paper/Yoshua Bengio and Olivier Delalleau - Justifying and Generalizing Contrastive Divergence - 2007.pdf:PDF},
  month = jan,
  owner = {stes},
  timestamp = {2016.06.26},
  title = {Justifying and Generalizing Contrastive Divergence},
  year = {2007}
}

@INPROCEEDINGS{Bergstra2010,
  author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric
	and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume
	and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
  title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
  year = {2010},
  month = jun,
  note = {Oral Presentation},
  abstract = {Theano is a compiler for mathematical expressions in Python that combines
	the convenience of NumPy’s syntax with the speed of optimized native
	machine language. The user composes mathematical expressions in a
	high-level description that mimics NumPy’s syntax and semantics,
	while being statically typed and functional (as opposed to imperative).
	These expressions allow Theano to provide symbolic differentiation.
	Before performing computation, Theano optimizes the choice of expressions,
	translates them into C++ (or CUDA for GPU), compiles them into dynamically
	loaded Python modules, all automatically. Common machine learning
	algorithms implemented with Theano are from 1.6× to 7.5× faster than
	competitive alternatives (including those implemented with C/C++,
	NumPy/SciPy and MATLAB) when compiled for the CPU and between 6.5×
	and 44× faster when compiled for the GPU. This paper illustrates
	how to use Theano, outlines the scope of the compiler, provides benchmarks
	on both CPU and GPU processors, and explains its overall design.},
  file = {:paper/Bergstra2013.pdf:PDF},
  location = {Austin, TX},
  owner = {sschneider},
  timestamp = {2015.09.30}
}

@OTHER{DiederikP.Kingma2014,
  author = {Diederik P. Kingma, Max Welling},
  file = {:home/stes/thesis/paper/Diederik P. Kingma, Max Welling - Auto-Encoding Variational Bayes - 2014.pdf:PDF},
  month = jan,
  owner = {sschneider},
  timestamp = {2016.07.20},
  title = {Auto-Encoding Variational Bayes},
  year = {2014}
}

@OTHER{Kingma2014,
  author = {Diederik P. Kingma, Max Welling},
  file = {:home/stes/thesis/paper/Diederik P. Kingma, Max Welling - Auto-Encoding Variational Bayes - 2014.pdf:PDF},
  month = jan,
  owner = {sschneider},
  timestamp = {2016.07.20},
  title = {Auto-Encoding Variational Bayes},
  year = {2014}
}

@MISC{Dieleman2015,
  author = {Sander Dieleman and Jan Schlüter and Colin Raffel and Eben Olson
	and Søren Kaae Sønderby and Daniel Nouri and Daniel Maturana and
	Martin Thoma and Eric Battenberg and Jack Kelly and Jeffrey De Fauw
	and Michael Heilman and diogo149 and Brian McFee and Hendrik Weideman
	and takacsg84 and peterderivaz and Jon and instagibbs and Dr. Kashif
	Rasul and CongLiu and Britefury and Jonas Degrave},
  title = {Lasagne: First release.},
  month = aug,
  year = {2015},
  doi = {10.5281/zenodo.27878},
  owner = {stes},
  timestamp = {2016.04.20},
  url = {http://dx.doi.org/10.5281/zenodo.27878}
}

@ARTICLE{Donoho2006,
  author = {D. L. Donoho},
  title = {Compressed sensing},
  journal = {IEEE Transactions on Information Theory},
  year = {2006},
  volume = {52},
  pages = {1289-1306},
  number = {4},
  month = {April},
  doi = {10.1109/TIT.2006.871582},
  issn = {0018-9448},
  keywords = {convex programming;data compression;image coding;image reconstruction;image
	sampling;image sensors;sparse matrices;transform coding;Euclidean
	space;convex optimization;general linear functional measurement;image
	reconstruction;nonadaptive nonpixel sampling;sensing compression;signal
	processing;sparse representation;transform coding;Compressed sensing;Data
	mining;Digital images;Image coding;Image reconstruction;Pixel;Signal
	processing;Size measurement;Transform coding;Vectors;Adaptive sampling;Basis
	Pursuit;Gel'fand;Quotient-of-a-Subspace theorem;almost-spherical
	sections of Banach spaces;eigenvalues of random matrices;information-based
	complexity;integrated sensing and processing;minimum;optimal recovery;sparse
	solution of linear equations}
}

@ARTICLE{Hinton2006a,
  author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  title = {A Fast Learning Algorithm for Deep Belief Nets},
  journal = {Neural computation},
  year = {2006},
  volume = {18},
  pages = {1527--1554},
  number = {7},
  month = jan,
  file = {:home/stes/thesis/paper/Geoffrey E. Hinton, Simon Osindero, Yee-Whye Teh - A Fast Learning Algorithm for Deep Belief Nets - 2006.pdf:PDF},
  owner = {stes},
  publisher = {MIT Press},
  timestamp = {2016.01.02}
}

@ARTICLE{Hinton2006b,
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  title = {Reducing the Dimensionality of Data with Neural Networks},
  journal = {Science},
  year = {2006},
  volume = {313},
  pages = {504-507},
  number = {5786},
  month = jan,
  abstract = {High-dimensional data can be converted to low-dimensional codes by
	training a multilayer neural network with a small central layer to
	reconstruct high-dimensional input vectors. Gradient descent can
	be used for fine-tuning the weights in such “autoencoder” networks,
	but this works well only if the initial weights are close to a good
	solution. We describe an effective way of initializing the weights
	that allows deep autoencoder networks to learn low-dimensional codes
	that work much better than principal components analysis as a tool
	to reduce the dimensionality of data.},
  doi = {10.1126/science.1127647},
  eprint = {http://www.sciencemag.org/content/313/5786/504.full.pdf},
  file = {:home/stes/thesis/paper/G. E. Hinton and R. R. Salakhutdinov - Reducing the Dimensionality of Data with Neural Networks - 2015.pdf:PDF},
  owner = {stes},
  timestamp = {2016.01.02},
  url = {http://www.sciencemag.org/content/313/5786/504.abstract}
}

@ARTICLE{Kamyshanska2015,
  author = {H. Kamyshanska and R. Memisevic},
  title = {The Potential Energy of an Autoencoder},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2015},
  volume = {37},
  pages = {1261-1273},
  number = {6},
  month = {June},
  doi = {10.1109/TPAMI.2014.2362140},
  issn = {0162-8828},
  keywords = {Boltzmann machines;learning (artificial intelligence);pattern classification;probability;RBM;class-specific
	autoencoders;contractive training;energy function;energy landscape;feature
	learning models;generative classifier;negative log-probability;potential
	energy;probabilistic model;reconstruction function;regularization
	procedures;restricted Boltzmann machine;sigmoid hidden units;training
	criterion;Analytical models;Data models;Potential energy;Principal
	component analysis;Probabilistic logic;Training;Vectors;Autoencoders;generative
	classification;representation learning;unsupervised learning}
}

@INPROCEEDINGS{Krizhevsky2012,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  year = {2012},
  editor = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
  pages = {1097--1105},
  publisher = {Curran Associates, Inc.},
  file = {:home/stes/thesis/paper/Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton - ImageNet Classification with Deep Convolutional Neural Networks.pdf:PDF},
  owner = {stes},
  timestamp = {2016.01.02},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@ARTICLE{Mousavi2015,
  author = {Ali Mousavi and Ankit B. Patel and Richard G. Baraniuk},
  title = {A Deep Learning Approach to Structured Signal Recovery},
  journal = {CoRR},
  year = {2015},
  volume = {abs/1508.04065},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/MousaviPB15},
  timestamp = {Tue, 01 Sep 2015 14:42:40 +0200},
  url = {http://arxiv.org/abs/1508.04065}
}

@ARTICLE{Palangi2015,
  author = {Hamid Palangi and Rabab K. Ward and Li Deng},
  title = {Distributed Compressive Sensing: {A} Deep Learning Approach},
  journal = {CoRR},
  year = {2015},
  volume = {abs/1508.04924},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/PalangiWD15},
  timestamp = {Tue, 01 Sep 2015 14:42:40 +0200},
  url = {http://arxiv.org/abs/1508.04924}
}

@ARTICLE{Simonyan14,
  author = {Simonyan, K. and Zisserman, A.},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal = {CoRR},
  year = {2014},
  volume = {abs/1409.1556},
  file = {:home/stes/thesis/paper/Simonyan, K. and Zisserman, A. - Very Deep Convolutional Networks for Large-Scale Image Recognition - 2015.pdf:PDF},
  owner = {stes},
  timestamp = {2016.01.06}
}

@ARTICLE{Szegedy15,
  author = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon
	Shlens and Zbigniew Wojna},
  title = {Rethinking the Inception Architecture for Computer Vision},
  journal = {CoRR},
  year = {2015},
  volume = {abs/1512.00567},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/SzegedyVISW15},
  owner = {stes},
  timestamp = {Sat, 02 Jan 2016 11:38:49 +0100},
  url = {http://arxiv.org/abs/1512.00567}
}

@OTHER{Nair2010,
  abstract = {Proceedings of the International Conference on Machine Learning 2010},
  author = {Vinod Nair, Geoffrey E. Hinton},
  file = {:home/stes/thesis/paper/Vinod Nair, Geoffrey E. Hinton - Rectified Linear Units Improve Restricted Boltzmann Machines - 2010.pdf:PDF},
  keywords = {Restricted Boltzmann Machines,rectified linear units},
  month = jan,
  owner = {stes},
  timestamp = {2016.06.26},
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  year = {2010}
}

@TECHREPORT{Wah2011,
  author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie,
	S.},
  title = {The Caltech-UCSD Birds-200-2011 Dataset},
  institution = {California Institute of Technology},
  year = {2011},
  number = {CNS-TR-2011-001}
}

@comment{jabref-meta: databaseType:bibtex;}

